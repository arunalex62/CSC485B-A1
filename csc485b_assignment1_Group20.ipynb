{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dclGnLGAgbtH",
        "outputId": "dc5c4ec8-d3ee-43c7-8903-4fcc292d16ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-u_a7s_ir\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-u_a7s_ir\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc4jupyter\n"
          ]
        }
      ],
      "source": [
        "# Load the extension that allows us to compile CUDA code in python notebooks\n",
        "# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVbDQthwogQF"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_types.h\"\n",
        "/**\n",
        " * A collection of commonly used data types throughout this project.\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <stdint.h> // uint32_t\n",
        "\n",
        "using element_t = uint32_t;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqET4uI2ggwf"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n",
        "/**\n",
        " * Standard macros that can be useful for error checking.\n",
        " * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <cuda.h>\n",
        "\n",
        "#define CUDA_CALL(exp)                                       \\\n",
        "    do {                                                     \\\n",
        "        cudaError res = (exp);                               \\\n",
        "        if(res != cudaSuccess) {                             \\\n",
        "            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n",
        "                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n",
        "           exit(EXIT_FAILURE);                               \\\n",
        "        }                                                    \\\n",
        "    } while(0)\n",
        "\n",
        "#define CHECK_ERROR(msg)                                             \\\n",
        "    do {                                                             \\\n",
        "        cudaError_t err = cudaGetLastError();                        \\\n",
        "        if(cudaSuccess != err) {                                     \\\n",
        "            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n",
        "                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE);                                      \\\n",
        "        }                                                            \\\n",
        "    } while (0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY0L7rKhoVaZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n",
        "/**\n",
        " * Functions for generating random input data with a fixed seed\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <random>  // for std::mt19937, std::uniform_int_distribution\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "\n",
        "/**\n",
        " * Generates and returns a vector of random uniform data of a given length, n,\n",
        " * for any integral type. Input range will be [0, 2n].\n",
        " */\n",
        "template < typename T >\n",
        "std::vector< T > generate_uniform( std::size_t n )\n",
        "{\n",
        "    // for details of random number generation, see:\n",
        "    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n",
        "    std::size_t random_seed = 20240916;  // use magic seed\n",
        "    std::mt19937 rng( random_seed );     // use mersenne twister generator\n",
        "    std::uniform_int_distribution<> distrib(0, 2 * n);\n",
        "\n",
        "    std::vector< T > random_data( n ); // init array\n",
        "    std::generate( std::begin( random_data )\n",
        "                 , std::end  ( random_data )\n",
        "                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n",
        "\n",
        "    return random_data;\n",
        "}\n",
        "\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJOKRZuCkDh2"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n",
        "#pragma once\n",
        "\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "namespace cpu {\n",
        "\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace cpu\n",
        "\n",
        "\n",
        "namespace gpu {\n",
        "\n",
        "void run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3lAuiBEhKjc"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n",
        "/**\n",
        " * CPU methods that the GPU should outperform.\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <algorithm> // std::sort()\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1      {\n",
        "namespace cpu     {\n",
        "\n",
        "/**\n",
        " * Simple solution that just sorts the whole array with a built-in sort\n",
        " * function and then resorts the last portion in the opposing order with\n",
        " * a second call to that same built-in sort function.\n",
        " */\n",
        "void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    std::sort( data, data + num_elements, std::less< element_t >{} );\n",
        "    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n",
        "}\n",
        "\n",
        "/**\n",
        " * Run the single-threaded CPU baseline that students are supposed to outperform\n",
        " * in order to obtain higher grades on this assignment. Times the execution and\n",
        " * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n",
        " * the functions takes the input by value so as to not perturb the original data\n",
        " * in place.\n",
        " */\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n",
        "{\n",
        "    auto const cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort( data.data(), switch_at, n );\n",
        "    auto const cpu_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"CPU Baseline time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace cpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjTbQ3EO2NwQ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n",
        "/**\n",
        " * The file in which you will implement your GPU solutions!\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "#include <limits> // for +infinity value\n",
        "#include <sstream>\n",
        "\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "    namespace csc485b {\n",
        "    namespace a1 {\n",
        "        namespace gpu {\n",
        "\n",
        "            /**\n",
        "             * The CPU baseline benefits from warm caches because the data was generated on\n",
        "             * the CPU. Run the data through the GPU once with some arbitrary logic to\n",
        "             * ensure that the GPU cache is warm too and the comparison is more fair.\n",
        "             */\n",
        "            __global__\n",
        "                void warm_the_gpu(element_t* data, std::size_t invert_at_pos, std::size_t num_elements)\n",
        "            {\n",
        "                int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "                // We know this will never be true, because of the data generator logic,\n",
        "                // but I doubt that the compiler will figure it out. Thus every element\n",
        "                // should be read, but none of them should be modified.\n",
        "                if (th_id < num_elements && data[th_id] > num_elements * 100)\n",
        "                {\n",
        "                    ++data[th_id]; // should not be possible.\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // Used this StackOverflow post for algorithm to find next power of 2.\n",
        "            // https://stackoverflow.com/questions/364985/algorithm-for-finding-the-smallest-power-of-two-thats-greater-or-equal-to-a-giv\n",
        "            // Commented out since we do not have to worry about input not being power of two.\n",
        "            //std::size_t next_power_of_two(element_t x) {\n",
        "            //    --x;\n",
        "            //    x |= x >> 1;\n",
        "            //    x |= x >> 2;\n",
        "            //    x |= x >> 4;\n",
        "            //    x |= x >> 8;\n",
        "            //    x |= x >> 16;\n",
        "            //    return x + 1;\n",
        "            //}\n",
        "\n",
        "            /**\n",
        "             * Your solution. Should match the CPU output.\n",
        "             */\n",
        "            __global__\n",
        "                void opposing_sort(element_t* data, std::size_t invert_at_pos, std::size_t num_elements)\n",
        "            {\n",
        "                element_t const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "                extern __shared__ element_t s_data[];\n",
        "\n",
        "                if (th_id < num_elements)\n",
        "                {\n",
        "                    // Load the data from global memory to shared memory\n",
        "                    // s_data[threadIdx.x] = data[th_id];\n",
        "\n",
        "                    // Perform Bitonic Sort but DIFFERENT\n",
        "                    for (std::size_t stage = 2; stage <= num_elements; stage <<= 1) {\n",
        "                        for (std::size_t step = stage >> 1; step > 0; step >>= 1) {\n",
        "                            // Determine the index of the element to compare with\n",
        "                            std::size_t partner = th_id ^ step;\n",
        "\n",
        "                            if (partner > th_id && partner < num_elements) {\n",
        "                                // Ascending if th_id and partner are in the same stage group (because stage shifts by 1 bit each time, this works nicely)\n",
        "                                bool ascending = (th_id & stage) == 0;\n",
        "\n",
        "                                // Compare and swap based on direction\n",
        "                                if (ascending) {\n",
        "                                    if (data[th_id] > data[partner]) {\n",
        "                                        element_t temp = data[th_id];\n",
        "                                        data[th_id] = data[partner];\n",
        "                                        data[partner] = temp;\n",
        "                                    }\n",
        "                                }\n",
        "                                else {\n",
        "                                    // Descending order\n",
        "                                    if (data[th_id] < data[partner]) {\n",
        "                                        element_t temp = data[th_id];\n",
        "                                        data[th_id] = data[partner];\n",
        "                                        data[partner] = temp;\n",
        "                                    }\n",
        "                                }\n",
        "                            }\n",
        "                            __syncthreads();\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "                    // Copy the sorted data back to global memory for this thread\n",
        "                    // data[th_id] = s_data[threadIdx.x];\n",
        "\n",
        "                    // Reverses array at invert position onwards.\n",
        "                    if (th_id >= invert_at_pos) {\n",
        "                        s_data[num_elements - 1 - th_id] = data[th_id];\n",
        "                        __syncthreads();\n",
        "                        int outOffset = blockDim.x * (blockIdx.x);\n",
        "                        int out = outOffset + th_id;\n",
        "                        data[out] = s_data[th_id - invert_at_pos];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            __global__\n",
        "            void opposing_sort_step( element_t * data, std::size_t num_elements, int j, int k, bool direction )\n",
        "            {\n",
        "                int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "                // Determine the index of the element to compare with\n",
        "                int partner = th_id ^ j;\n",
        "\n",
        "                if (partner > th_id && partner < num_elements) {\n",
        "                    // Ascending if th_id and partner are in the same stage group (because stage shifts by 1 bit each time, this works nicely)\n",
        "                    // If direction is false, the boolean value for ascending is flipped.\n",
        "                    bool ascending = ((th_id & k) == 0) ^ !direction;\n",
        "\n",
        "                    // Compare and swap based on direction\n",
        "                    if (ascending) {\n",
        "                        if (data[th_id] > data[partner]) {\n",
        "                            element_t temp = data[th_id];\n",
        "                            data[th_id] = data[partner];\n",
        "                            data[partner] = temp;\n",
        "                        }\n",
        "                    } else {\n",
        "                        if (data[th_id] < data[partner]) {\n",
        "                            element_t temp = data[th_id];\n",
        "                            data[th_id] = data[partner];\n",
        "                            data[partner] = temp;\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "\n",
        "            /**\n",
        "             * Performs all the logic of allocating device vectors and copying host/input\n",
        "             * vectors to the device. Times the opposing_sort() kernel with wall time,\n",
        "             * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n",
        "             */\n",
        "            void run_gpu_soln(std::vector< element_t > data, std::size_t switch_at, std::size_t n)\n",
        "            {\n",
        "\n",
        "                // Check if input is not a power of 2, and then pad input to a power of 2.\n",
        "                // Code has been commented out as test cases will only be powers of 2.\n",
        "                // if (n & (n-1)) {\n",
        "                //    std::size_t next_power_of_2 = next_power_of_two(n);\n",
        "                //    for (std::size_t i = 0; i < (next_power_of_2 - n); ++i) {\n",
        "                //        data.push_back(std::numeric_limits<element_t>::max()-1);\n",
        "                //    }\n",
        "                //    n = next_power_of_2;\n",
        "                //}\n",
        "                std::size_t const threads_per_block = 1024;\n",
        "                std::size_t const num_blocks = (n + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "                // Allocate arrays on the device/GPU\n",
        "                element_t* d_data;\n",
        "                cudaMalloc((void**)&d_data, sizeof(element_t) * n);\n",
        "                CHECK_ERROR(\"Allocating input array on device\");\n",
        "\n",
        "                // Copy the input from the host to the device/GPU\n",
        "                cudaMemcpy(d_data, data.data(), sizeof(element_t) * n, cudaMemcpyHostToDevice);\n",
        "                CHECK_ERROR(\"Copying input array to device\");\n",
        "\n",
        "                // Warm the cache on the GPU for a more fair comparison\n",
        "                warm_the_gpu << < num_blocks, threads_per_block >> > (d_data, switch_at, n);\n",
        "\n",
        "                // Time the execution of the kernel that you implemented\n",
        "                auto const kernel_start = std::chrono::high_resolution_clock::now();\n",
        "                auto const smem_size = threads_per_block * sizeof(element_t);\n",
        "\n",
        "                // Run with a single thread block if input size allows\n",
        "                if (n <= 1024) {\n",
        "                    opposing_sort << < num_blocks, threads_per_block, smem_size >> > (d_data, switch_at, n);\n",
        "                }\n",
        "                else {\n",
        "                    // Otherwise do inter-block synchronization in CPU code\n",
        "                    int j, k;\n",
        "                    // Major step\n",
        "                    for (k = 2; k <= n; k <<= 1) {\n",
        "                        // Minor step\n",
        "                        for(j = k >> 1; j > 0; j >>= 1) {\n",
        "                        opposing_sort_step<<< num_blocks, threads_per_block>>>( d_data, n, j, k, true);\n",
        "                        }\n",
        "                    }\n",
        "                    // Re-sort last quarter in reverse order\n",
        "                    for (k = 2; k <= n; k <<= 1) {\n",
        "                        // Minor step\n",
        "                        for(j = k >> 1; j > 0; j >>= 1) {\n",
        "                        opposing_sort_step<<< num_blocks, threads_per_block>>>( d_data + switch_at, n - switch_at, j, k, false);\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "                // Remove positive infinities from padded input if original input was not a power of two.\n",
        "                // Commented out since input is guaranteed to be a power of two in the test cases.\n",
        "                 //std::vector<element_t> result;\n",
        "                 //for (std::size_t i = 0; i < n; ++i) {\n",
        "                 //    if (data[i] != std::numeric_limits<element_t>::max() - 1) {\n",
        "                 //        result.push_back(data[i]);\n",
        "                 //    }\n",
        "                 //}\n",
        "\n",
        "                auto const kernel_end = std::chrono::high_resolution_clock::now();\n",
        "                //for (auto const x : result) std::cout << x << \" \"; std::cout << std::endl;\n",
        "                CHECK_ERROR(\"Executing kernel on device\");\n",
        "\n",
        "                // After the timer ends, copy the result back, free the device vector,\n",
        "                // and echo out the timings and the results.\n",
        "                cudaMemcpy(data.data(), d_data, sizeof(element_t) * n, cudaMemcpyDeviceToHost);\n",
        "                CHECK_ERROR(\"Transferring result back to host\");\n",
        "                cudaFree(d_data);\n",
        "                CHECK_ERROR(\"Freeing device memory\");\n",
        "\n",
        "                std::cout << \"GPU Solution time: \"\n",
        "                    << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n",
        "                    << \" ns\" << std::endl;\n",
        "\n",
        "                //for (auto const x : data) std::cout << x << \" \"; std::cout << std::endl;\n",
        "                std::ostringstream buffer;\n",
        "\n",
        "                // Simulate writing many strings to the buffer\n",
        "                for (std::size_t i = 0; i < n; ++i) {\n",
        "                    buffer << data[i] << \" \";\n",
        "                }\n",
        "                buffer << '\\n';\n",
        "\n",
        "                // Print the entire buffer content to console with a single std::cout call\n",
        "                std::cout << buffer.str();\n",
        "            }\n",
        "\n",
        "        } // namespace gpu\n",
        "    } // namespace a1\n",
        "} // namespace csc485b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRvVeK-QifnZ"
      },
      "outputs": [],
      "source": [
        "%%cuda_group_save -g \"source\" -n \"main.cu\"\n",
        "/**\n",
        " * Driver for the benchmark comparison. Generates random data,\n",
        " * runs the CPU baseline, and then runs your code.\n",
        " */\n",
        "\n",
        "#include <cstddef>  // std::size_t type\n",
        "#include <iostream> // std::cout, std::endl\n",
        "#include <vector>\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "#include \"data_generator.h\"\n",
        "#include \"data_types.h\"\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "int main()\n",
        "{\n",
        "    std::size_t const n = 8;\n",
        "    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n",
        "\n",
        "    auto data = csc485b::a1::generate_uniform< element_t >( n );\n",
        "    csc485b::a1::cpu::run_cpu_baseline( data, switch_at, n );\n",
        "    csc485b::a1::gpu::run_gpu_soln( data, switch_at, n );\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7F0eVsGjUNp",
        "outputId": "bf647162-bc67-4da0-912a-b7b6319b0710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Baseline time: 473 ns\n",
            "0 0 3 6 10 13 15 13 \n",
            "GPU Solution time: 13806 ns\n",
            "0 0 3 6 10 13 15 13 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Yqomwu6WsP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}